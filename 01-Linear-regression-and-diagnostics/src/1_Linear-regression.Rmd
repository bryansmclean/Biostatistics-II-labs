---
title: "LINEAR REGRESSION AND DIAGNOSTICS IN R"
author: "Bryan McLean"
date: "January 2021"
output: html_document
---

*Note: parts of this lab were taken from open-source web tutorials.*\

### A bit of background...

The movie Moneyball (2011) focuses on the “quest for the secret of success in baseball”. It follows a low-budget team, the Oakland Athletics, who believed that underused statistics, such as a player’s ability to get on base, betterpredict the ability to score runs than typical statistics like home runs, RBIs (runs batted in), and batting average. Obtaining players who excelled in these underused statistics turned out to be much more affordable for the team.

In this lab we’ll be looking at data from all 30 Major League Baseball teams and examining the linear relationship between runs scored in a season and a number of other player statistics. Our aim will be to summarize these relationships both graphically and numerically in order to find which variable, if any, helps us best predict a team’s runs scored in a season.\

### Load the data into R

```{r}
setwd("/Users/mclean/Box/UNCG_Teaching/Biostats_II/labs/1_Linear-regressions_Model-checking")
load("./data/mlb11.rdata")
# This data set contains team-wide summaries for production statistics
# There should be 30 teams and 11 statistics
```

### Examine the data

```{r}
# First, make sure the matrix was read into R in full:
# (i.e., that it has the dimensions we expect)
nrow(mlb11)  # number of teams
ncol(mlb11) - length(which(colnames(mlb11) == 'team'))  # number of stats

# There are several options for printing or summarizing data tables in R.
# The head() function prints the first rows of a table
head(mlb11)

# Alternatively, we can summarize across rows and columns (R prints only several row summaries by default)
summary(mlb11)
```

In addition to runs scored, there are seven traditionally used variables in the data set: at-bats, hits, home runs, batting average, strikeouts, stolen bases, and wins. There are also three newer variables: on-base percentage, slugging percentage, and on-base plus slugging.\

**As a baseball manager**, one way to maximize number of runs scored could be to just increase the number of at-bats. Lets plot this relationship to explore whether more at-bats might lead to more runs scored (using the variable at_bats as the predictor, i.e., the independent variable). Does the relationship look linear? 


```{r}
plot(mlb11$at_bats, mlb11$runs)
```

### Run a Formal Linear Model

The relationship above appears linear, but we can test that using a simple linear regression between the two variables. 

R's standard linear regression function is lm(). Calls take the form y ~ x, with an optional second argument specifying the data frame that R should look in to find the model variables.

```{r}
m1 <- lm(runs ~ at_bats, data = mlb11)
```

The output of lm() is an object that contains all of the information we need about the linear model we just fit. We can quickly access key information using the summary() function.

```{r}
summary(m1)
```

Let’s consider the model summary piece by piece. 

**1)** First, the formula you called to set up the model is shown at the top. 

**2)** After the formula is the five-number summary of the residuals. We'll examine this in more detail later. 

**3)** The “coefficients” table is shown next. This type of table is a common part of many modeling outputs in R. Its first row displays the y-intercept for the model. Its second row contains the coefficients of at_bats. With this information so far, we can write down the least squares regression line for the entire linear model:

*ŷ =−2789.242 9 +0.6305∗atbat*

**4)** Another component of the model summary we are typically interested in is the multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 37.3% of the variability in runs is explained by at-bats. The adjusted R2 value is a modification of the multiple R2 (we'll also come back to that later).

**5)** One final important component of the model summary is the statistical significance. Remember that the significance level of each model variable (rows in the table) is printed at the end of the rows. The significance of the ENTIRE model is printed on the bottom row of the summary table (showing that this simple model is highly significant).

In this model, since our sole predictor (at_bats) was significant, its no surprise that the entire model is also significant at the alpha = 0.05 level.
\
\

## MODEL DIAGNOSTICS

To assess whether any linear model is reliable, one needs to check the following assumptions to make sure they hold:

**(1)** linearity (relationship between X and the mean of Y is linear)
**(2)** constant variability, or homoscedasticity (variance of Y is the same for any value of X)
**(3)** normality (distribution of Y is normal at any value of X)
  
We already have reason to think that (1) is true, as our model summary above confirms that number of at bats ("at_bats") is strongly predictive of number of runs ("runs"). 

However, far from all of the variation in "runs" is explained by "at_bats" (check the R2 values above again). R's linear model object contains lots of other information about this residual variation (i.e., that variation not explained by at_bats), exploring the structure of which is important for model diagnostics.

### Visualizing residual variation

To begin, explore features of our model using the attributes() function (tip: this is useful for exploring all kinds of R objects!).

```{r}
attributes(m1)
```

Prior to anything more detailed, it is often instructive to create a simple plot just to help visualize differences between the model predictions and the actual values (i.e., to visualize the RESIDUAL variation). To do this, we can make use of the "fitted_values" and "residuals" attributes of the model. 

Also, we can make use of the handy abline() function to plot the least-squares (i.e., best-fit) line.

Im also going to plot some line segments that connect the actual and predicted (fitted) values using the segments() function.

```{r}
plot(mlb11$at_bats, mlb11$runs)
abline(m1, col = "dodgerblue")      # plotting the model
segments(
  mlb11$at_bats,         # X1 values
  m1$fitted.values,      # predicted Y values (the model fit)
  mlb11$at_bats,         # X2 values
  mlb11$runs,            # real Y values (the actual data)
  col = 'dodgerblue',
  lty = 2
  )
```

### Tests of linearity and homoscedasticity

Two additional plots allow us to check assumptions (1) and (2) listed above.

The first is a plot of residuals versus predicted values. If the data don't meet the linearity assumption, we would expect to see residuals that are very large (big positive or negative values). What we want to see is residuals centered on 0 (standardized values less than -2 or greater than 2 are deemed problematic). 
This plot also allows us to test the homoscedasticity assumption. What we want to see is no pattern in residuals along different values of X; there should be a constant level of variation around the Y = 0 line.

The second plot below is just a histogram of the residuals themselves, which is also useful for testing homoscedasticity.

```{r}
par(mfrow = c(1,2))
plot(m1 $residuals ~ m1$fitted.values, col = 'dodgerblue')  # plot of residuals vs. fitted
abline(h = 0, lty = 3)  # add a horizontal dashed line at y = 0
hist(m1$residuals, col = 'white', border = 'dodgerblue')  # histogram of residuals
```

### Tests of normality

The final assumption of our linear model is normality (#3 above), and it is evaluated based on the residuals using a QQ-plot. This plot compares the residuals to "ideal" normal observations. Observations lying along the 1:1 line in the QQ-plot indicate that the normality assumption holds. (Note: the plot() function applied to a model object generates a series of diagnostic plots, so we can just choose the second of these.)

```{r}
par(mfrow = c(1,1))
plot(m1, which = 2, col = 'dodgerblue')  # the q-q plot
```
\
\
\

## THIS WEEK'S ASSIGNMENT

At this point, you should be familiar with the syntax and workflow necessary to set up a simple, BIVARIATE linear model in R using data stored in R's memory. You should also be familiar with performing basic diagnostics on your model to make sure it meets THREE major assumptions of LINEARITY, CONSTANT VARIABILITY (homoscedasticity), and NORMALITY.

**Your assignment for this portion of the lab is to construct a bivariate linear model for the relationship between at_bats (independent) and strikeouts (dependent).** You previously found that more at_bats lead to more runs scored (a good thing), but it is reasonable to think that more at_bats also results in more striekouts (a bad thing).\
\

#### **DO THE FOLLOWING:**
**1. Construct a linear model of strikeouts vs. at_bats**\
\
**2. Create a 4-panel (2x2) plot that contains the following model and diagnostics, in this order (starting from upper left):**\
        a). scatterplot of strikeouts (Y) vs. at_bats (X), with the best-fit line from the model added\
        b). scatterplot of the residuals vs. fitted values for the model (as above for hits)\
        c). histogram of the residuals (as above for hits)\
        d). Q-Q plot (as above for hits)\
(hint: to set up this 4-panel plot, use the par() function prior to plotting and the mfrow() argument)\
\
**3. Create an output file for R's standard model SUMMARY object for your model**\
(hint: refresh yourself on the summary() function above, and also check out the help page for the capture.output() function)\
\
**4. Save both files (plot and model table) using the naming convention e.g., lastname_lab1_plot1**\
\
**5. Store both outputs to upload to Canvas in combination with next week's lab**\

\
\
\
rmarkdown::render('./1_Linear-regression.Rmd')

