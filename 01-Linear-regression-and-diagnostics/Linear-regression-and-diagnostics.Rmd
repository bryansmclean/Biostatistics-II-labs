---
title: "LINEAR REGRESSION AND DIAGNOSTICS IN R"
author: "Bryan McLean"
date: "January 2021; updated Jan. 2022"
output:
  pdf_document: default
  html_document: default
---

*Note: parts of this lab were taken from open-source web tutorials.*\

## CONSTRUCTING A LINEAR MODEL

### A bit of background...

The movie Moneyball (2011) focuses on the “quest for the secret of success in baseball”. It follows a low-budget team, the Oakland Athletics, who believed that underused statistics, such as a player’s ability to get on base, can predict ability to score runs better than typical statistics like home runs, RBIs (runs batted in), and batting average. Obtaining players who excelled in these underused statistics turned out to be much more affordable for the team.

In this lab we’ll be looking at data from all 30 Major League Baseball teams and examining the linear relationship between runs scored in a season and a number of other player statistics. Our aim will be to summarize these relationships both graphically and numerically in order to find which variable, if any, helps us best predict a team’s runs scored in a season.\

### Load the data into R

```{r}
load("./data/mlb11.rdata")
# This data set contains team-wide summaries for production statistics
# There should be 30 teams and 11 statistics
```

### Examine the data

```{r}
# First, make sure the matrix was read into R in full:
# (i.e., that it has the dimensions we expect)
nrow(mlb11)  # number of teams
ncol(mlb11) - length(which(colnames(mlb11) == 'team'))  # number of stats

# There are several options for printing or summarizing data tables in R.
# The head() function prints the first rows of a table
head(mlb11)

# Alternatively, we can summarize across rows and columns (R prints only several row summaries by default)
summary(mlb11)
```

In addition to runs scored, there are seven traditionally used variables in the data set: at-bats, hits, home runs, batting average, strikeouts, stolen bases, and wins. There are also three newer variables: on-base percentage, slugging percentage, and on-base plus slugging.\

**As a baseball manager**, one way to maximize number of runs scored could be to just increase the number of at-bats (that is, get less outs!). Lets plot this relationship to explore whether more at-bats might lead to more runs scored (using the variable at_bats as the predictor, i.e., the independent variable). Does the relationship look linear? 

```{r}
plot(mlb11$at_bats, mlb11$runs)
```

### Construct a formal linear model

The relationship above appears linear, but we can do better than eyeballing. Let's test it using a simple linear regression between the two variables. 

Remember that linear regression seeks to explain the variation in a response variable by the variation in one or more predictor variables, assuming a linear relationship and several other things (which are covered below). R's standard linear regression function is *lm()*. Calls take the form y ~ x, with an optional second argument specifying the data frame that R should look in to find the model variables.

```{r}
m1 <- lm(runs ~ at_bats, data = mlb11)
```

The output of *lm()* is a model object that contains all of the information we need about the linear model we just fit. We can quickly access key information about the model using the summary() function. Most of the models we will run in R this semester have similar model structure - so take some time to get familiar with it!

```{r}
summary(m1)
```

### Interpret a model object

Let’s consider the model summary piece by piece. 

**1)** **Call** The first item listed in the model sumary is the formula you called. This is very useful as a reminder of the model you actually ran. 

**2)** **Residuals** After the formula is the five-number summary of the residuals. This describes the distribution of leftover variation not accounted for by the model. We'll examine this in more detail later.

**3)** **Coefficients** The “coefficients” table is shown next. Get familiar with the layout, because this is a common part of many modeling objects in R. Any linear regression has basic properties of y-intercept and slope, and that is what is described here. The first row displays the y-intercept for the model. The second row contains the coefficients of *at_bats*. Using just this information, we can write down the least squares regression line for the entire linear model:

*ŷ =−2789.242 9 +0.6305∗atbat*

The statistical significance of each model term is listed in the final column. **Some practical interpretation** is that the significant Intercept indicates that the observed intercept is different than zero, and the significant slope indicates that the observed slope is different than zero.

**4)** **R-squared** Another component of the model summary we are typically interested in is the multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, about 37.3% of the variability in runs is explained by at-bats. The adjusted R2 value is a modification of the multiple R2 (we'll also come back to that later).

**5)** **Model significance** One final important component of the model summary is its statistical significance. Remember that the significance level of each model variable (rows in the table) is printed at the end of the rows. The significance of the ENTIRE model is printed on the bottom row of the summary table (showing that this simple model is highly significant).

Since our sole predictor (at_bats) had a slope significantly different than zero, its no surprise that the entire model is also significant at the alpha = 0.05 level.
\
\

## MODEL DIAGNOSTICS

### Assumptions of linear models

To assess whether any linear model is reliable, you should always check the following assumptions to make sure they hold:

**(1)** **linearity** (relationship between X and the mean of Y is linear) \
**(2)** **constant variability**, or **homoscedasticity** (variance of Y is the same for any value of X) \
**(3)** **normality** (distribution of Y is normal at any value of X) \
  
We already have reason to think that (1) is true, as our model summary above confirms that number of at bats ("at_bats") is strongly predictive of number of runs ("runs") given the statisticall significant slope term. 

However, far from all of the variation in "runs" is explained by "at_bats" (check the R2 values above again). R's linear model object contains lots of other information about this residual variation (i.e., that variation not explained by at_bats), and exploring the structure of the residuals is the central part of model diagnostics.

### Visualize residual variation

To begin, explore model features using the *attributes()* function (tip: this is useful for exploring all kinds of R objects!).

```{r}
attributes(m1)
```

Prior to anything more detailed, it is often instructive to create a simple plot just to help visualize differences between the model predictions and the actual values (i.e., to visualize the RESIDUAL variation). To do this, we can make use of the "fitted_values" and "residuals" attributes of the model. 

Also, we can make use of the handy abline() function to plot the least-squares (i.e., best-fit) line.

Im also going to plot some line segments that connect the actual and predicted (fitted) values using the segments() function, for visualization purposes.

```{r}
plot(mlb11$at_bats, mlb11$runs)
abline(m1, col = "dodgerblue")      # plotting the model
segments(
  mlb11$at_bats,         # X1 values
  m1$fitted.values,      # predicted Y values (the model fit)
  mlb11$at_bats,         # X2 values
  mlb11$runs,            # real Y values (the actual data)
  col = 'dodgerblue',
  lty = 2
  )
```

*Do the residuals at all values of Y (runs) seem to have equal variance at all values of X (at_bats)?*
*Do the residuals of Y seem mormally distributed about X?*

### Test linearity and homoscedasticity

Two additional plots allow us to check assumptions (1) and (2) listed above.

The first is a plot of residuals versus predicted values. If the data don't meet the linearity assumption, we would expect to see residuals that are very large (big positive or negative values). What we want to see is residuals centered on 0 (standardized values less than -2 or greater than 2 are deemed problematic). 
This plot also allows us to test the homoscedasticity assumption. What we want to see is **no** pattern in residuals of Y at different values of X; there should be a constant level of variation around the Y = 0 line.

The second plot below is just a histogram of the residuals themselves, which is also useful for testing homoscedasticity.

```{r}
par(mfrow = c(1,2))
plot(m1 $residuals ~ m1$fitted.values, col = 'dodgerblue')  # plot of residuals vs. fitted
abline(h = 0, lty = 3)  # add a horizontal dashed line at y = 0
hist(m1$residuals, col = 'white', border = 'dodgerblue')  # histogram of residuals
```

*Looking at the left plot, are there equal amounts of residual variation in Y, at all values of X?* \
*Looking at the right plot, does this residual variation in Y follow a roughly normal distribution?* \

### Test normality

A final assumption of our linear model is normality (#3 above), and it is evaluated based on the residuals using a QQ-plot. This plot compares the residuals to "ideal" normal observations. Observations lying along the 1:1 line in the QQ-plot indicate that the normality assumption holds. (Note: the plot() function applied to a model object generates a series of diagnostic plots, so we can just choose the second of these.)

```{r}
par(mfrow = c(1,1))
plot(m1, which = 2, col = 'dodgerblue')  # the q-q plot
```
\
\
\

## THIS WEEK'S ASSIGNMENT

At this point, you should be familiar with the syntax and workflow necessary to set up a simple, bivariate linear model in R using data stored in R's memory, and interpret the resulting model object. You should also be familiar with performing basic diagnostics on your model to make sure it meets THREE major assumptions of *linearity*, *constant variability (homoscedasticity)*, and *normality*.

**Your assignment is to construct a bivariate linear model for the relationship between at_bats (independent) and strikeouts (dependent).** You previously found that more at_bats lead to more runs scored (a good thing), but it is reasonable to think that more at_bats also results in more striekouts (a bad thing).\
\

#### **Do the following:**
**1. Construct a linear model of strikeouts vs. at_bats in R**\
\
**2. Create a 4-panel (2x2) plot that contains the following plots and diagnostics, in this order (clockwise from upper left):**\
        a). scatterplot of strikeouts (Y) vs. at_bats (X), with the best-fit line from the model added\
        b). scatterplot of the residuals vs. fitted values for the model (as above for hits)\
        c). histogram of the residuals (as above for hits)\
        d). Q-Q plot (as above for hits)\
(hint: to set up this 4-panel plot, use the par() function prior to plotting and the mfrow() argument)\
\
**3. Create an output file for R's standard model object for your model**\
(hint: make sure to use the summary() function as we did above, and use the capture.output() function to write this to a text (.txt) file. check that function's help page for more info.)\
\
**4. Save both files (4-panel plot and model table) using the naming convention (e.g., lastname_lab1_plot1)**\
\
**5. Upload both files to Canvas**\

\
\
\
rmarkdown::render('Linear-regression-and-diagnostics.Rmd')

